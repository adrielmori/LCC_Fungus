{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql8-1Yy5uKwR"
      },
      "source": [
        "\n",
        "\n",
        "### INITIAL\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2wxeGlyKoaB5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "! git clone https://github.com/artsousa/HyperSI\n",
        "! pip install -r HyperSI/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDIWzadAg2mb",
        "outputId": "15c798f9-7419-4ef2-a320-a2731ad7a547"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import pprint\n",
        "import importlib\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from google.colab import drive\n",
        "\n",
        "repo_path = '/content/HyperSI'\n",
        "sys.path.insert(0, repo_path)\n",
        "\n",
        "from source import (\n",
        "    utils,\n",
        "    pipeline,\n",
        "    hsiroutine,\n",
        "    sample\n",
        ")\n",
        "\n",
        "\n",
        "importlib.reload(utils)\n",
        "importlib.reload(sample)\n",
        "importlib.reload(pipeline)\n",
        "importlib.reload(hsiroutine)\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5k2GH5L1J-I"
      },
      "source": [
        "### Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae-WG9t3ldRi"
      },
      "outputs": [],
      "source": [
        "# import spectral as sp\n",
        "# sp.settings.envi_support_nonlowecase_params = True\n",
        "\n",
        "# teste = sp.open_image('/content/drive/Shareddrives/HSI_Fungos/Carrapatos/data/Fungo_Manisopliae_IP119_1_221221-163308/capture/Fungo_Manisopliae_IP119_1_221221-163308.hdr')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqSzfWOP8S38"
      },
      "source": [
        "#### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzNbRnX2Akwv",
        "outputId": "9a5ccb8f-cef5-4003-cbbe-3783596e4f10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Candida_albicans_48h_210716-114834\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(1423, 320, 256)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import spectral as sp\n",
        "\n",
        "samples_names = {}\n",
        "\n",
        "data_ = \"/content/drive/MyDrive/DATA/FUNGOS/data/\"\n",
        "key = \"/content/drive/MyDrive/DATA/FUNGOS/data/Candida_albicans_48h_210716-114834/\"\n",
        "sample_name = key.split('/')[-2]\n",
        "\n",
        "print(sample_name)\n",
        "image = sp.open_image(os.path.join(data_, sample_name, 'capture' , sample_name + '.hdr'))\n",
        "\n",
        "sample = image.load()\n",
        "\n",
        "sample.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4MJItDxYd3f"
      },
      "outputs": [],
      "source": [
        "# idx = np.array([1, 2, 2, 2, 1, 1, 1, 1])\n",
        "# print(hsiroutine.HsiRoutine.realIdx(idx, 2))\n",
        "\n",
        "# ind, rm = hsiroutine.HsiRoutine.sum_idx_array(hsiroutine.HsiRoutine.realIdx(idx, 2))\n",
        "# hsiroutine.HsiRoutine.rev_idx_array(ind, rm)\n",
        "\n",
        "# teste = np.arange(24).reshape(3, 4, -1)\n",
        "\n",
        "# print(teste[:, :, 0])\n",
        "# print(teste[:, :, 1])\n",
        "# print(teste.shape, '\\n\\n', )\n",
        "\n",
        "# teste = teste.transpose(2, 0, 1)\n",
        "# print(teste, teste.shape, '\\n\\n', )\n",
        "\n",
        "# teste = teste.transpose(1, 2, 0)\n",
        "# print(teste[:, :, 0])\n",
        "# print(teste[:, :, 1])\n",
        "# print(teste.shape, '\\n\\n', )\n",
        "\n",
        "# teste = teste.transpose(2, 0, 1)\n",
        "# teste_matrix = teste.T.reshape((teste.shape[1] * teste.shape[2], teste.shape[0]), order='F')\n",
        "# teste_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6MNYyRnvuQe"
      },
      "source": [
        "#### Functions and classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8W5l3-gMvwzn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import glob\n",
        "import time\n",
        "import torch\n",
        "import traceback\n",
        "import numpy as np\n",
        "import pickle as pkl\n",
        "\n",
        "\n",
        "from typing import Dict, Any\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def cluster_image(\n",
        "    hypercube: np.array, \n",
        "    pipeline: pipeline.HsiPipeline, \n",
        "    sample_name: str,\n",
        "    **kwargs\n",
        "):\n",
        "\n",
        "    matrix = pipeline._signal_filter(\n",
        "        cube=hypercube, \n",
        "        order=kwargs['golay_order'], \n",
        "        window=kwargs['golay_window'], \n",
        "        dv=kwargs['golay_deriv'])\n",
        "    \n",
        "    print('P--/ --', end='')\n",
        "\n",
        "    rows, cols = hypercube.shape[1:]\n",
        "    cube = pipeline.routine.matrix2hsi(matrix, rows, cols)\n",
        "\n",
        "    idx = pipeline.routine.removeBg(\n",
        "        matrix[:, 100:250], pcs=kwargs['pcs'], k_clusters=kwargs['k_clusters']\n",
        "    ) + 1\n",
        "\n",
        "    print('R--/')\n",
        "\n",
        "    image = cube[150, :, :]\n",
        "\n",
        "    out_1 = pipeline.routine.getCluster(image, idx, 1, (1, 0, 0))\n",
        "    out_2 = pipeline.routine.getCluster(image, idx, 2, (0, 0, 1))\n",
        "\n",
        "    figure(figsize=(6, 6), dpi=100)\n",
        "    # plt.subplot(1, 2, 1)\n",
        "    plt.imshow(out_1)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    figure(figsize=(6, 6), dpi=100)\n",
        "    # plt.subplot(1, 2, 2)\n",
        "    plt.imshow(out_2)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    cluster = input('which image represents the sample? 1 or 2')\n",
        "    plt.close('all')\n",
        "\n",
        "    ind, rm = pipeline.routine.sum_idx_array(\n",
        "        pipeline.routine.realIdx(idx, int(cluster))\n",
        "    )\n",
        "    \n",
        "    sample_cluster = pipeline.routine.rev_idx_array(ind, rm)\n",
        "    out_1 = pipeline.routine.getCluster(image, sample_cluster, 0, (1, 1, 1))\n",
        "    \n",
        "    figure(figsize=(6, 6), dpi=100)\n",
        "    plt.imshow(pipeline.routine.rgbscale(out_1))\n",
        "    plt.axis('off')\n",
        "    plt.savefig(f\"{sample_name}_bgremoved.png\")\n",
        "    \n",
        "    plt.close()\n",
        "    \n",
        "    return sample_cluster\n",
        "\n",
        "\n",
        "class FungusDataset(Dataset):\n",
        "    def __init__(self, folder: str, \n",
        "                 samples: Dict, \n",
        "                 task: int = 0,\n",
        "                 load_all: bool = True,\n",
        "                 dest_folder: str = '',\n",
        "                 first_time: bool = True,\n",
        "                 process_images: bool = False,\n",
        "                 **kwargs):\n",
        "\n",
        "        self.data = {}\n",
        "        self.task = task\n",
        "        self.folder = folder\n",
        "        self.samples = samples\n",
        "        self.dest_folder = dest_folder\n",
        "        self.pipeline = pipeline.HsiPipeline(data_folder=folder, samples=samples)\n",
        "\n",
        "        self.__get_wavelength()\n",
        "        \n",
        "        if load_all:\n",
        "            if first_time:\n",
        "                self.__get_samples()\n",
        "            elif process_images:\n",
        "                self.__load_pkl(\n",
        "                    preprocess_=True, method='kmeans', **kwargs\n",
        "                )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(list(self.samples.keys()))\n",
        "\n",
        "    def __getitem__(self, sample_name):\n",
        "        return utils.Utils.load_hsisample(\n",
        "            path=self.folder, name=sample_name, folder='capture'\n",
        "        )\n",
        "\n",
        "    def __show_sample(self, sample_name: str, save: bool = False):\n",
        "        \n",
        "        try:\n",
        "            print(f\"Sample: {sample_name} --\", end='')\n",
        "            fungus = utils.Utils.load_hsisample(\n",
        "                path=self.folder, name=sample_name, folder='capture'\n",
        "            )\n",
        "            \n",
        "            print('L--/ --', end='')\n",
        "\n",
        "            image = fungus.normalized[150, :, :]\n",
        "\n",
        "            rows, cols = fungus.normalized.shape[1:]\n",
        "            image = self.pipeline.routine.getCluster(\n",
        "                image, fungus.sample_cluster, 0, (1, 1, 1)\n",
        "            )\n",
        "\n",
        "            print(f'P--/ --{rows * cols}--/ --{(rows, cols)}--')\n",
        "            figure(figsize=(6, 6), dpi=100)\n",
        "            plt.imshow(self.pipeline.routine.rgbscale(image))\n",
        "            plt.axis('off')\n",
        "\n",
        "            if save:\n",
        "                plt.savefig(f\"{sample_name}_vis.png\")\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            traceback.print_tb(e.__traceback__)\n",
        "            print(f\"Sample {sample_name} not loaded... \\n {e}\")\n",
        "\n",
        "\n",
        "    def visualize_images(self,):\n",
        "\n",
        "        for hsisample in self.pipeline.samples:\n",
        "            sample_name = hsisample.split(\"/\")[-2]\n",
        "\n",
        "            self.__show_sample(sample_name, save=False)\n",
        "\n",
        "\n",
        "    def __load_pkl(self, preprocess_: bool = True, \n",
        "                   method: str = 'kmeans', **kwargs):\n",
        "\n",
        "        for hsisample in self.pipeline.samples:\n",
        "            \n",
        "            sample_name = hsisample.split(\"/\")[-2]\n",
        "            print(f\"Sample: {sample_name} --\", end='')\n",
        "\n",
        "            try:\n",
        "                fungus = utils.Utils.load_hsisample(\n",
        "                    path=self.folder, name=sample_name, folder='capture'\n",
        "                )\n",
        "\n",
        "                print(f\"L--/ --\", end='')\n",
        "                if preprocess_:\n",
        "                    fungus.sample_cluster = cluster_image(\n",
        "                        fungus.normalized, \n",
        "                        self.pipeline, \n",
        "                        self.dest_folder + sample_name, **kwargs\n",
        "                    )\n",
        "\n",
        "                    fungus.save()\n",
        "                    del fungus\n",
        "\n",
        "                # image = fungus.normalized[150, :, :]\n",
        "                \n",
        "                # print(f\"\\n\\nSample: {sample_name} {type(fungus)} -- \" + \n",
        "                #       f\"{fungus.normalized.shape} -- {image.shape}\")\n",
        "                \n",
        "                # figure(figsize=(6, 6), dpi=100)\n",
        "                # plt.imshow(image, cmap='gray')\n",
        "                # plt.axis('off')\n",
        "                # plt.show()\n",
        "\n",
        "            except Exception as e:\n",
        "                traceback.print_tb(e.__traceback__)\n",
        "                print(f\"Sample {sample_name} not loaded... \\n {e}\")\n",
        "\n",
        "        return\n",
        "\n",
        "    def __get_samples(self,):\n",
        "\n",
        "        print('Loading samples...')\n",
        "        for hsisample in self.pipeline.samples:\n",
        "            sample_name = hsisample.split(\"/\")[-2]\n",
        "\n",
        "            try:\n",
        "                \n",
        "                st_time = time.time()\n",
        "                fungus = sample.Sample(\n",
        "                    self.pipeline.folder, sample_name\n",
        "                )\n",
        "                darkref = sample.Sample(\n",
        "                    self.pipeline.folder, sample_name, sample_prefix=\"DARKREF_\"\n",
        "                )\n",
        "                whiteref = sample.Sample(\n",
        "                    self.pipeline.folder, sample_name, sample_prefix=\"WHITEREF_\"\n",
        "                )\n",
        "\n",
        "                fungus.normalized = self.pipeline.routine.raw2mat(\n",
        "                    image=fungus, dark=darkref, white=whiteref, inplace=False\n",
        "                )\n",
        "\n",
        "                fungus.image = None\n",
        "                fungus.save()\n",
        "\n",
        "                print(f\"Sample: {sample_name} -- {time.time() - st_time}s --\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Sample {sample_name} not loaded... \\n {e}\")\n",
        "            \n",
        "        print('Done...')\n",
        "\n",
        "        return\n",
        "\n",
        "    def __get_wavelength(self):\n",
        "        self.wavelength = {}\n",
        "\n",
        "        for hsisample in self.pipeline.samples:\n",
        "            sample_name = hsisample.split(\"/\")[-2]\n",
        "\n",
        "            path = os.path.join(\n",
        "                self.pipeline.folder, sample_name, \"capture\", sample_name + \".hdr\"\n",
        "            )\n",
        "\n",
        "            self.wavelength[sample_name] = self.pipeline.routine.get_wavelength(\n",
        "                folder=self.folder, sample=sample_name, spectral_range=(0, -1)\n",
        "            )\n",
        "\n",
        "        unique = True\n",
        "        original = 0\n",
        "        for key in list(self.wavelength.keys()):\n",
        "            if type(original) is int:\n",
        "                original = self.wavelength[key]\n",
        "\n",
        "            if sum((original - self.wavelength[key])[0]):\n",
        "                unique = False\n",
        "                print('not all wavelength are the same')\n",
        "                break\n",
        "\n",
        "        self.wavelength = original"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28JRifeggpZh"
      },
      "source": [
        "#### load .raw samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fNjEsgWmzQa8"
      },
      "outputs": [],
      "source": [
        "data_path = \"/content/drive/Shareddrives/HSI_Fungos/Fungos/data\"\n",
        "# data_path = '/content/drive/Shareddrives/HSI_Fungos/Carrapatos/data'\n",
        "\n",
        "fungus_samples = glob.glob(data_path + \"/*/\")\n",
        "env_sample = [name for i, name in enumerate(fungus_samples) if \"Meio\" in name]\n",
        "fungus_samples.remove(env_sample[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PzlFXrrKEGQ-"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "# fungus_samples = {\n",
        "#     f\"{data_path}/Aspergillus_niger_14d_210716-114349/\": [1],\n",
        "#     f\"{data_path}/Candida_albicans_48h_210716-114834/\": [2],\n",
        "#     f\"{data_path}/Aspergillus_niger_30d_210716-114543/\": [3],\n",
        "#     f\"{data_path}/Aspergillus_niger_48h_210716-113748/\": [4],\n",
        "#     f\"{data_path}/Aspergillus_niger_30d_210716-114626/\": [5],\n",
        "#     f\"{data_path}/Aspergillus_niger_96h_210716-114013/\": [6],\n",
        "#     f\"{data_path}/Aspergillus_terreus_48h_210716-112704/\": [7],\n",
        "#     f\"{data_path}/Aspergillus_terreus_14d_210716-113405/\": [8],\n",
        "#     f\"{data_path}/Aspergillus_terreus_30d_210716-113623/\": [9],\n",
        "#     f\"{data_path}/Aspergillus_terreus_96h_210716-112952/\": [10],\n",
        "#     f\"{data_path}/Candida_albicans_14d_210716-115055/\": [11],\n",
        "#     f\"{data_path}/Candida_albicans_30d_210716-115154/\": [12],\n",
        "#     f\"{data_path}/Candida_albicans_96h_210716-114946/\": [13],\n",
        "#     f\"{data_path}/Fusarim_chlamydosporums_14d_210716-111105/\": [14],\n",
        "#     f\"{data_path}/Fusarim_chlamydosporums_48h_210716-110338/\": [15],\n",
        "#     f\"{data_path}/Fusarim_chlamydosporums_30d_210716-111246/\": [16],\n",
        "#     f\"{data_path}/Fusarim_chlamydosporums_96h_210716-110832/\": [17],\n",
        "#     f\"{data_path}/Penicillium_spp_30d_210716-112221/\": [18],\n",
        "#     f\"{data_path}/Penicillium_spp_14d_210716-112032/\": [19],\n",
        "#     f\"{data_path}/Penicillium_spp_48h_210716-111545/\": [20],\n",
        "#     f\"{data_path}/Penicillium_spp_96h_210716-111758/\": [21],\n",
        "# }\n",
        "\n",
        "fungus_samples = {sample: [i + 1] for i, sample in enumerate(fungus_samples)}\n",
        "sample_names = [name.split(\"/\")[-2] for name in list(fungus_samples.keys())]\n",
        "sample_species = list(\n",
        "    set(name.split(\"_\")[0] + \"_\" + name.split(\"_\")[1] for name in sample_names)\n",
        ")\n",
        "sample_species = [(group, i + 1) for i, group in enumerate(sample_species)]\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "# Training config\n",
        "# Maturation time: 48h, 96h, 14d, 30d\n",
        "\n",
        "training_cfg = {'val': '48h',\n",
        "                'test': '96h'}\n",
        "\n",
        "for key in list(fungus_samples.keys()): ## group by samples\n",
        "    for group in sample_species:\n",
        "        if group[0] in key:\n",
        "            fungus_samples[key].append(group[1])\n",
        "\n",
        "            # Define training and val samples\n",
        "            for cfg in list(training_cfg.keys()):\n",
        "                if training_cfg[cfg] in key:\n",
        "                    if cfg == 'val':\n",
        "                        res_ = 1\n",
        "                        break\n",
        "                    elif cfg == 'test':\n",
        "                        res_ = 2\n",
        "                        break \n",
        "                else:\n",
        "                    res_ = 0\n",
        "\n",
        "            fungus_samples[key].append(res_)\n",
        "\n",
        "            break\n",
        "\n",
        "# ------------------------------------------------------------------------------\n",
        "\n",
        "preprocess_args = {'pcs': 2,\n",
        "                   'k_clusters': 2,\n",
        "                   'golay_order': 2,\n",
        "                   'golay_deriv': 1,\n",
        "                   'golay_window': 25}\n",
        "\n",
        "# for key in list(fungus_samples.keys()):\n",
        "#     for group in [sample_species[0]]:\n",
        "#         if group[1] == fungus_samples[key][1]:\n",
        "#             print(key, fungus_samples[key])\n",
        "#             break\n",
        "        # print('\\n\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O-nI7OTn07_T"
      },
      "outputs": [],
      "source": [
        "dataset = FungusDataset(\n",
        "    folder=data_path, samples=fungus_samples,\n",
        "    load_all=False, \n",
        "    first_time=False,\n",
        "    dest_folder='/content/figures/',\n",
        "    process_images=False,\n",
        "    **preprocess_args\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2vx_QPC9ZCk4"
      },
      "outputs": [],
      "source": [
        "# dataset.samples ## dict per task\n",
        "# cube.normalized ## hypercube complete\n",
        "# cube.sample_cluster ## index of sample pixels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WsshEfJys2nw"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from typing import List\n",
        "import matplotlib\n",
        "import warnings\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def plot_matrix(\n",
        "    dataset: Dataset,\n",
        "    matrix: np.array,\n",
        "    desc: str,\n",
        "    desc_size: np.array,\n",
        "    group: int,\n",
        "    fig: matplotlib.figure.Figure,\n",
        "    axes: matplotlib.axes.Axes,\n",
        "):\n",
        "\n",
        "    plotargs = {\n",
        "        \"label\": desc,\n",
        "        \"linewidth\": 2,\n",
        "        # 'color': dataset.pipeline.utils.colors[str(group)]\n",
        "    }\n",
        "    axes.set_ylabel(\"Pseudo Absorbance\")\n",
        "    axes.set_xlabel(\"Wavelength (nm)\")\n",
        "    axes.set_xlabel(axes.get_xlabel(), dataset.pipeline.properties[\"fontproperties\"])\n",
        "    axes.set_ylabel(axes.get_ylabel(), dataset.pipeline.properties[\"fontproperties\"])\n",
        "\n",
        "    x_axis = dataset.wavelength\n",
        "    ind = np.linspace(0, x_axis.shape[1] - 1, num=5, dtype=int)\n",
        "\n",
        "    axes.set_xticks(ind)\n",
        "    axes.set_xticklabels(x_axis[0, ind])\n",
        "\n",
        "    mean_matrix = np.mean(matrix, axis=0)\n",
        "\n",
        "    return axes.plot(np.arange(mean_matrix.shape[0]), mean_matrix, **plotargs)\n",
        "\n",
        "\n",
        "def plot_samples(dataset: Dataset, task: int, mean_group: bool, **kwargs):\n",
        "    groups = get_group(dataset, task)\n",
        "\n",
        "    # ------ Handle plots\n",
        "    # plots = []\n",
        "    # fig, axes = plt.subplots(figsize=(12, 6), dpi=1000)\n",
        "    # plt.rcParams.update({'font.size': 10, 'legend.frameon': False})\n",
        "\n",
        "    # for key in list(groups.keys()):\n",
        "    #     print(f\"group_key: {key} group: {groups[key]} \\n\")\n",
        "\n",
        "    #     group_data, descs = group_samples(group=groups[key], dataset=dataset)\n",
        "    #     print(f\"shape: {group_data.shape}, description: {descs}\")\n",
        "\n",
        "    #     for desc in descs:\n",
        "    #         if not mean_group:\n",
        "    #             plots.append(plot_matrix(\n",
        "    #                 dataset, group_data[desc[1][0]:desc[1][1] + 1],\n",
        "    #                 desc[0], desc[1], key, fig, axes\n",
        "    #             ))\n",
        "    #         else:\n",
        "    #             plots.append(plot_matrix(\n",
        "    #                 dataset, group_data,\n",
        "    #                 desc[0], np.array([]), key, fig, axes\n",
        "    #             ))\n",
        "\n",
        "    #         axes.legend(loc='center left',\n",
        "    #                     bbox_to_anchor=(1, 0.5),\n",
        "    #                     prop={'size': 12,})\n",
        "\n",
        "    #     break\n",
        "\n",
        "    # ------ Handle plots\n",
        "\n",
        "    # plot_matrix(dataset, group, samples)\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "def group_samples_plot(group: Dict, dataset: Dataset):\n",
        "\n",
        "    samples = []\n",
        "    data_group = np.array([])\n",
        "    for hsisample in group:\n",
        "\n",
        "        sample_name = hsisample[1].split(\"/\")[-2]\n",
        "\n",
        "        sample_data = dataset[sample_name]\n",
        "        ind, _ = dataset.pipeline.routine.sum_idx_array(\n",
        "            dataset.pipeline.routine.realIdx(sample_data.sample_cluster, 1)\n",
        "        )\n",
        "\n",
        "        ind, _ = train_test_split(ind, test_size=0.7, shuffle=True)\n",
        "\n",
        "        matrix = dataset.pipeline.routine.hsi2matrix(sample_data.normalized)\n",
        "        matrix = dataset.pipeline.routine.normalize_mean(matrix[ind, :])\n",
        "\n",
        "        if not data_group.shape[0]:\n",
        "            data_group = np.array([]).reshape(0, matrix.shape[1])\n",
        "\n",
        "        samples.append(\n",
        "            (sample_name, data_group.shape[0] + np.array([0, matrix.shape[0] - 1]))\n",
        "        )\n",
        "        data_group = np.concatenate([data_group, matrix], axis=0)\n",
        "\n",
        "    return data_group, samples\n",
        "\n",
        "\n",
        "def get_group(dataset: Dataset, task: int):\n",
        "\n",
        "    groups = {}\n",
        "    samples = dataset.samples\n",
        "    for hsisample in list(samples.keys()):\n",
        "        sample_name = \"_\".join(hsisample.split(\"/\")[-2].split(\"_\")[0:3])\n",
        "\n",
        "        if samples[hsisample][task] not in list(groups.keys()):\n",
        "            groups[samples[hsisample][task]] = [(sample_name, hsisample)]\n",
        "        else:\n",
        "            groups[samples[hsisample][task]].append((sample_name, hsisample))\n",
        "\n",
        "    return groups\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def groups2dataframe(groups: Dict, dataset: Dataset):\n",
        "\n",
        "    gp_df = pd.DataFrame()\n",
        "\n",
        "    for key in dataset.samples:  # dict keys\n",
        "        line = {}\n",
        "        columns = [\"sample\"] + [str(i) for i in range(len(dataset.samples[key]))]\n",
        "\n",
        "        for column in columns:\n",
        "            if column == \"sample\":\n",
        "                line[column] = key\n",
        "            else:\n",
        "                line[column] = int(dataset.samples[key][int(column)])\n",
        "\n",
        "        gp_df = gp_df.append(line, ignore_index=True)\n",
        "\n",
        "    return gp_df\n",
        "\n",
        "\n",
        "def _get_sample(\n",
        "    sample_name: str,\n",
        "    dataset: Dataset,\n",
        "    preprocess: bool = False,\n",
        "    sub_sampling: float = 0.3,\n",
        "):\n",
        "\n",
        "    sample_data = dataset[sample_name]\n",
        "    ind, _ = dataset.pipeline.routine.sum_idx_array(\n",
        "        dataset.pipeline.routine.realIdx(sample_data.sample_cluster, 1)\n",
        "    )\n",
        "\n",
        "    if sub_sampling > 0.0:\n",
        "        ind, _ = train_test_split(ind, test_size=1 - sub_sampling, shuffle=True)\n",
        "\n",
        "    matrix = dataset.pipeline.routine.hsi2matrix(sample_data.normalized)\n",
        "\n",
        "    if preprocess:\n",
        "        pass\n",
        "    else:\n",
        "        matrix = dataset.pipeline.routine.normalize_mean(matrix[ind, :])\n",
        "\n",
        "    return matrix\n",
        "\n",
        "\n",
        "def get_group_samples(group: List, output: int, sf: float):\n",
        "\n",
        "    group_matrix = None\n",
        "\n",
        "    for hsisample in group:\n",
        "        sample_name = hsisample.split(\"/\")[-2]\n",
        "        matrix = _get_sample(sample_name, dataset, preprocess=False, sub_sampling=sf)\n",
        "\n",
        "        if not (type(group_matrix) == np.ndarray):\n",
        "            group_matrix = matrix\n",
        "        else:\n",
        "            group_matrix = np.concatenate([group_matrix, matrix], axis=0)\n",
        "\n",
        "    return group_matrix, np.ones(group_matrix.shape[0], dtype=int) * output\n",
        "\n",
        "\n",
        "def split_train_val(dataset: Dataset, task: int, split: int = -1, **kwargs):\n",
        "\n",
        "    train, val, test = [], [], []\n",
        "    groups_dict = get_group(dataset, task)\n",
        "    groups_dframe = groups2dataframe(groups_dict, dataset)\n",
        "\n",
        "    for gk in list(groups_dict.keys()):\n",
        "        task_df = groups_dframe[groups_dframe[str(task)] == float(gk)]\n",
        "\n",
        "        sets = task_df[str(split)].unique()\n",
        "        split_df = None\n",
        "        if split > 0:\n",
        "            split_df = {\n",
        "                \"train\": list(task_df[task_df[str(split)] == 0.0][\"sample\"].values),\n",
        "                \"teste\": list(task_df[task_df[str(split)] == 2.0][\"sample\"].values),\n",
        "            }\n",
        "\n",
        "            if 1.0 in sets:\n",
        "                split_df[\"valid\"] = list(task_df[task_df[str(split)] == 1.0][\"sample\"].values),\n",
        "                X_valid, y_valid = get_group_samples(split_df[\"valid\"][0], gk, sf=0.2)\n",
        "                val.append((X_valid, y_valid))\n",
        "\n",
        "            X_train, y_train = get_group_samples(split_df[\"train\"], gk, sf=0.2)        \n",
        "            X_teste, y_teste = get_group_samples(split_df[\"teste\"], gk, sf=0.2)\n",
        "        \n",
        "        test.append((X_teste, y_teste))\n",
        "        train.append((X_train, y_train))\n",
        "\n",
        "    def _concatenate(\n",
        "        groups: List[np.array],\n",
        "    ):\n",
        "        group_y = None\n",
        "        group_matrix = None\n",
        "\n",
        "        for group in groups:\n",
        "\n",
        "            if not (type(group_matrix) == np.ndarray):\n",
        "                group_y = group[1]\n",
        "                group_matrix = group[0]\n",
        "            else:\n",
        "                group_y = np.concatenate([group_y, group[1]], axis=0)\n",
        "                group_matrix = np.concatenate([group_matrix, group[0]], axis=0)\n",
        "\n",
        "        return group_matrix, group_y\n",
        "\n",
        "    return _concatenate(train), _concatenate(val) if len(val) > 0 else (), _concatenate(test)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "train, vall, test = split_train_val(dataset=dataset, task=1, split=2, **preprocess_args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I13qp_JH8VSo",
        "outputId": "a7d29763-cb1e-427b-f53e-3bdf66c5d998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ]
        }
      ],
      "source": [
        "from sklearn import (\n",
        "    svm,\n",
        "    discriminant_analysis,\n",
        ")\n",
        "import seaborn as sns\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "seed = 42\n",
        "models = [\n",
        "    # discriminant_analysis.LinearDiscriminantAnalysis(\n",
        "    #     covariance_estimator=None,\n",
        "    #     n_components=None,\n",
        "    #     priors=None, shrinkage=None,\n",
        "    #     solver='svd',store_covariance=False,\n",
        "    #     tol=0.0001)\n",
        "    svm.SVC(decision_function_shape=\"ovo\", kernel=\"rbf\", verbose=True, max_iter=-1)\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    x_test, y_test = shuffle(test[0], test[1], random_state=42)\n",
        "    x_valid, y_valid = shuffle(vall[0], vall[1], random_state=42)\n",
        "    x_train, y_train = shuffle(train[0], train[1], random_state=42)\n",
        "\n",
        "    cls = model.fit(x_train, y_train)\n",
        "\n",
        "    pred_valid = cls.predict(x_valid)\n",
        "    pred_teste = cls.predict(x_test)\n",
        "\n",
        "    print(\"Validation: \")\n",
        "    print(classification_report(y_valid, pred_valid), \"\\n\\n\")\n",
        "\n",
        "    print(\"Test Set: \")\n",
        "    print(classification_report(y_test, pred_teste), \"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xmfEGkTUed8"
      },
      "outputs": [],
      "source": [
        "pred_teste = cls.predict(x_test)\n",
        "display(classification_report(y_test, pred_teste))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGharLZ32BtB"
      },
      "outputs": [],
      "source": [
        "# for key in fungus_samples:\n",
        "#     sample_name = key.split('/')[-2]\n",
        "    \n",
        "#     pkl_file = os.path.join(key, 'capture', sample_name + '.pkl')\n",
        "#     if os.path.exists(pkl_file):\n",
        "#         os.remove(pkl_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4095aEhTjczX",
        "outputId": "522f01d0-6125-4493-df31-d9c8f743a987"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\tData Source:   '/content/drive/MyDrive/DATA/FUNGOS/data/Aspergillus_niger_14d_210716-114349/capture/Aspergillus_niger_14d_210716-114349.raw'\n",
              "\t# Rows:           1424\n",
              "\t# Samples:         320\n",
              "\t# Bands:           256\n",
              "\tInterleave:        BIL\n",
              "\tQuantization:  16 bits\n",
              "\tData format:    uint16"
            ]
          },
          "execution_count": 150,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fungus.image"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "uqSzfWOP8S38",
        "p6MNYyRnvuQe"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "e06ff7da33dc9620448857a90ad8b5f428f0d573d205a934d2841c8aee45ea32"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
